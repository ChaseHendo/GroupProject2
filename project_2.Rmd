---
title: "Project 2"
author: "Austin Simeone"
date: "4/7/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(readr)
library(lubridate)
library(corrplot)
library(Hmisc)
library(caret)
library(randomForest)
library(mlbench)
library(doParallel)
library(MASS)
library(car)
```

```{r}
set.seed(69)
#Read in the data and make initial transformations and ID
kickstart_df<- read_csv("ks-projects-201801.csv")[,-c(1,2,5,7,9,13,14)]
#removed name, and kickstarter money metrics and currency

head(kickstart_df)

levels(as.factor(kickstart_df$state))

summary(kickstart_df)

table(kickstart_df$state)

#only keep success or failure
kickstart_df <- subset(kickstart_df, state == 'successful' | state == 'failed')

#remove old launched dates
kickstart_df <- subset(kickstart_df, year(launched) >= 2009)

#date time to just date
kickstart_df$launched <- date(kickstart_df$launched)

fitControl <- trainControl(## 10-fold CV
  method = "repeatedcv",
  number = 10,
  repeats = 3
)

cl <- makePSOCKcluster(8)
registerDoParallel(cl)

logit_model_no_params <- train(state ~ ., data = kickstart_df[,-c(3,4)],method="glm",family=binomial(), trControl=fitControl)

stopCluster(cl)

print(logit_model_no_params)

kickstart_df$length_of_campaign <- as.numeric(kickstart_df$deadline - kickstart_df$launched)

table(kickstart_df$length_of_campaign)

hist(as.numeric(kickstart_df$length_of_campaign)) #around 30 and around 60 right tail with a lot of 30 and 60


kickstart_df$launch_month <- month(kickstart_df$launched)

#kickstart_df$launch_year <- year(kickstart_df$launched)
# we donn't need this for prediction

# kickstart_df %>% 
#   group_by(launch_year) %>%
#   summarise(percentage = sum(state == 'successful')/(sum(state == 'successful')+ sum(state == 'failed')))

#maybe make years from 2009

kickstart_df %>% 
  group_by(launch_month) %>%
  summarise(percentage = sum(state == 'successful')/(sum(state == 'successful')+ sum(state == 'failed')))

names(kickstart_df)

# levels(as.factor(kickstart_df$category))
# 
# levels(as.factor(kickstart_df$main_category))

lapply(kickstart_df, class)

kickstart_df[,c(1,2,5,7,10)]<- lapply(kickstart_df[,c(1,2,5,7,10)], factor)

kickstart_df<- kickstart_df[,-c(3,4)]

kickstart_df<- kickstart_df[complete.cases(kickstart_df),]

kickstart_df_clean_no_main <- kickstart_df[,-2]

kickstart_df_clean_yes_main <- kickstart_df[,-1]

kickstart_matrix <- as.matrix(kickstart_df)

kickstart_matrix_clean_no_main <- kickstart_matrix[,-2]

kickstart_matrix_clean_yes_main <- kickstart_matrix[,-1]

#DATA IS CLEAN
```


```{r}

cookd(logit_model_no_params)

leverage.plots(logit_model_no_params)

#check assumptions
#no correlation between continuous 
rcorr(kickstart_matrix[,c(4,6,7)])

ggplot(kickstart_df, aes(main_category, backers)) + geom_boxplot()
#close median far variance
ggplot(kickstart_df, aes(main_category, usd_goal_real)) + geom_boxplot() 
#close median far variance
```

```{r}
fitControl <- trainControl(## 10-fold CV
  method = "repeatedcv",
  number = 10,
  repeats = 3
)

cl <- makePSOCKcluster(8)
registerDoParallel(cl)

logit_model <- train(state ~ ., data = kickstart_df,method="glm",family=binomial(),
             trControl=fitControl)

logit_model_main <- train(state ~ ., data = kickstart_df_clean_yes_main,method="glm",family=binomial(),
             trControl=fitControl)

logit_model_cat <- train(state ~ ., data = kickstart_df_clean_no_main,method="glm",family=binomial(),
             trControl=fitControl)


stopCluster(cl)

print(logit_model_cat)
print(logit_model)
print(logit_model_main)
#selection model
#decision tree
```

```{r}
fitControl <- trainControl(## 10-fold CV
  method = "cv",
  number = 10
)

cl <- makePSOCKcluster(8)
registerDoParallel(cl)

mtry <- 8
tunegrid <- expand.grid(.mtry=mtry)
metric <- "Accuracy"
rf_default <- train(state ~ ., data = na.omit(kickstart_df),method="rf",metric = metric, tuneGrid = tunegrid , trControl=fitControl)

stopCluster(cl)

summary(rf_default)
plot(rf_default)

cl <- makePSOCKcluster(8)
registerDoParallel(cl)

fitControl <- control(method = 'cv', number=10, search = "grid",savePredictions = TRUE)
set.seed(69)
tunegrid <- expand.grid(.mtry=c(1:8), .ntree = c(1000,1500,2000,2500))
rf_gridsearch <- train(state ~ ., data = kickstart_df,method="rf",metric = metric, tuneGrid = tunegrid , trControl=fitControl)

stopCluster(cl)

summary(rf_gridsearch)
plot(rf_gridsearch)

```

```{r}
lda_fit <- train(state ~., data = kickstart_df, method = 'lda', trControl = fitControl)
print(lda_fit)
summary(lda_fit)

```

We first checked for influential points through leverage plots and Cook's D calculation. We found X outliers and removed them after the first interation of our Logistic regression. Next we performed the Hosmer-LemeShow test for goodness of fit. With a p-value of X we reject a lack of fit for this model. However, we wanted to compare the results to that of other models and increase accuracy. For prediciton purposes a model with less features will tend to fair worse than that of a model with all features included. Also, we only have 8 total variables in our final dataset, each essential to understanding the outcome. Thus, to accomodate the requirments of the project, we tested against varying number of features for some of the no logistic models.In testing our logisitc regression model accuracy, we compared the results of a full feature with with those of, less feature, added feature, and non-parametic. The results can be found in table #. We used accuracy as our metric due to its ubiquity in classification. We used an LDA model, as well Random Forest to test the claim that these more complicated models will perform better than simple logistic regression. Random Forest algorithms tend to perform best on these types of binary datasets. The logistic regression had over 90% accuracy, but with over 300,000 datapoints we assumed that the "machine learning" algorithms could work better.
